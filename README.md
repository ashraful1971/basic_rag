# Local RAG with ChromaDB

A lightweight Retrieval-Augmented Generation (RAG) system built with **TypeScript**, **ChromaDB**, and **Ollama**. This project uses local embeddings and a local LLM to answer questions based on private data in `info.txt`.

## ðŸš€ Quick Start

### 1. Prerequisites
* **Node.js**: v18 or higher.
* **Ollama**: [Download here](https://ollama.com/) to run models locally.
* **ChromaDB**: Ensure you have a Chroma instance running.

### 2. Setup Models
Pull the required models using Ollama:
```bash
# The LLM (Brain)
ollama pull granite3-dense:2b

# The Embedding Model (Context Search)
ollama pull nomic-embed-text
```

### 3. Installation
```bash
npm install
```

### 4. Data Preparation
Add your knowledge base text to: src/data/info.txt

### 5. Running the Project
```bash
# Start development mode (with auto-reload and path aliases)
npm run dev
```

## ðŸ›  Project Structure
```bash
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/          # Database & Client configurations
â”‚   â”œâ”€â”€ services/        # Logic for embeddings, querying, and LLM calls
â”‚   â”œâ”€â”€ data/            # Your raw .txt/data files
â”‚   â””â”€â”€ index.ts         # Main entry point (Chat Loop)
â”œâ”€â”€ tsconfig.json        # TS configuration with @/ path aliases
â””â”€â”€ package.json         # Modern scripts using tsx and tsc-alias
```
